{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31cc9809",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-02T18:18:16.848986Z",
     "iopub.status.busy": "2025-11-02T18:18:16.848752Z",
     "iopub.status.idle": "2025-11-02T18:18:16.852907Z",
     "shell.execute_reply": "2025-11-02T18:18:16.852225Z"
    },
    "papermill": {
     "duration": 0.011068,
     "end_time": "2025-11-02T18:18:16.854073",
     "exception": false,
     "start_time": "2025-11-02T18:18:16.843005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff571511",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:18:16.862639Z",
     "iopub.status.busy": "2025-11-02T18:18:16.862108Z",
     "iopub.status.idle": "2025-11-02T18:19:56.300625Z",
     "shell.execute_reply": "2025-11-02T18:19:56.299754Z"
    },
    "papermill": {
     "duration": 99.444094,
     "end_time": "2025-11-02T18:19:56.302023",
     "exception": false,
     "start_time": "2025-11-02T18:18:16.857929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2400 image tiles and 2400 label tiles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2400/2400 [01:38<00:00, 24.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Preprocessing complete. Saved 2400 image-label pairs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FIXED PREPROCESSING: match Image_###.tif with Label_###.tif\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"/kaggle/input/private-data-1/patches\")\n",
    "IMG_DIR = DATA_DIR / \"Images\"\n",
    "LBL_DIR = DATA_DIR / \"Labels\"\n",
    "\n",
    "OUT_DIR = Path(\"/kaggle/working/preprocessed_data\")\n",
    "(OUT_DIR / \"images\").mkdir(parents=True, exist_ok=True)\n",
    "(OUT_DIR / \"pseudo_labels\").mkdir(exist_ok=True)\n",
    "(OUT_DIR / \"importance_maps\").mkdir(exist_ok=True)\n",
    "(OUT_DIR / \"similarity_maps\").mkdir(exist_ok=True)\n",
    "\n",
    "IMG_SIZE = 512\n",
    "\n",
    "def normalize_image(img):\n",
    "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img.astype(np.float32) / 255.0\n",
    "\n",
    "def normalize_label(label):\n",
    "    label = cv2.resize(label, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "    label = label.astype(np.float32)\n",
    "    if label.ndim == 3:\n",
    "        label = cv2.cvtColor(label, cv2.COLOR_BGR2GRAY)\n",
    "    label /= label.max() if label.max() != 0 else 1.0\n",
    "    return label\n",
    "\n",
    "def compute_importance_map(label):\n",
    "    weights = label / (np.sum(label) + 1e-6)\n",
    "    return weights\n",
    "\n",
    "def compute_similarity_map(img):\n",
    "    gray = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "    grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    edge_strength = np.sqrt(grad_x**2 + grad_y**2)\n",
    "    similarity_map = np.exp(-edge_strength / (np.max(edge_strength) + 1e-6))\n",
    "    return similarity_map\n",
    "\n",
    "# --- MAIN LOOP ---\n",
    "image_files = sorted(list(IMG_DIR.glob(\"*.tif\")))\n",
    "label_files = sorted(list(LBL_DIR.glob(\"*.tif\")))\n",
    "\n",
    "print(f\"Found {len(image_files)} image tiles and {len(label_files)} label tiles...\")\n",
    "processed = 0\n",
    "\n",
    "for img_path in tqdm(image_files, desc=\"Preprocessing\"):\n",
    "    # Extract numeric ID from image name\n",
    "    img_id = ''.join([ch for ch in img_path.stem if ch.isdigit()])\n",
    "    if not img_id:\n",
    "        continue\n",
    "\n",
    "    lbl_path = LBL_DIR / f\"Label_{img_id}.tif\"\n",
    "    if not lbl_path.exists():\n",
    "        continue\n",
    "\n",
    "    img = cv2.imread(str(img_path))\n",
    "    lbl = cv2.imread(str(lbl_path), cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None or lbl is None:\n",
    "        continue\n",
    "\n",
    "    img_norm = normalize_image(img)\n",
    "    lbl_norm = normalize_label(lbl)\n",
    "    imp_map = compute_importance_map(lbl_norm)\n",
    "    sim_map = compute_similarity_map(img_norm)\n",
    "\n",
    "    np.save(OUT_DIR / \"images\" / f\"tile_{img_id}.npy\", img_norm)\n",
    "    np.save(OUT_DIR / \"pseudo_labels\" / f\"tile_{img_id}.npy\", lbl_norm)\n",
    "    np.save(OUT_DIR / \"importance_maps\" / f\"tile_{img_id}.npy\", imp_map)\n",
    "    np.save(OUT_DIR / \"similarity_maps\" / f\"tile_{img_id}.npy\", sim_map)\n",
    "\n",
    "    processed += 1\n",
    "\n",
    "print(f\"âœ… Preprocessing complete. Saved {processed} image-label pairs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4883f26b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:19:56.371256Z",
     "iopub.status.busy": "2025-11-02T18:19:56.370683Z",
     "iopub.status.idle": "2025-11-02T18:20:00.258823Z",
     "shell.execute_reply": "2025-11-02T18:20:00.258220Z"
    },
    "papermill": {
     "duration": 3.923703,
     "end_time": "2025-11-02T18:20:00.260160",
     "exception": false,
     "start_time": "2025-11-02T18:19:56.336457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PyTorch Dataset for Weakly Supervised Segmentation\n",
    "# ------------------------------------------------------------\n",
    "# Loads preprocessed data:\n",
    "#   images/\n",
    "#   pseudo_labels/\n",
    "#   importance_maps/\n",
    "#   similarity_maps/\n",
    "# ------------------------------------------------------------\n",
    "# Author: thxshsv (Anirudh International LLC)\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "DATA_DIR = Path(\"/kaggle/working/preprocessed_data\")\n",
    "\n",
    "# ============================================================\n",
    "# DATASET CLASS\n",
    "# ============================================================\n",
    "class PotsdamWeaklyDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, augment=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (Path): path to the directory containing subfolders:\n",
    "                             images/, pseudo_labels/, importance_maps/, similarity_maps/\n",
    "            transform: optional torchvision transform\n",
    "            augment (bool): whether to apply random augmentations\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.img_paths = sorted((self.data_dir / \"images\").glob(\"*.npy\"))\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        name = img_path.stem\n",
    "\n",
    "        img = np.load(img_path)\n",
    "        label = np.load(self.data_dir / \"pseudo_labels\" / f\"{name}.npy\")\n",
    "        imp = np.load(self.data_dir / \"importance_maps\" / f\"{name}.npy\")\n",
    "        sim = np.load(self.data_dir / \"similarity_maps\" / f\"{name}.npy\")\n",
    "\n",
    "        # --- Normalize and convert to tensors ---\n",
    "        img = torch.from_numpy(img.transpose(2, 0, 1)).float()  # (C,H,W)\n",
    "        label = torch.from_numpy(label).unsqueeze(0).float()     # (1,H,W)\n",
    "        imp = torch.from_numpy(imp).unsqueeze(0).float()         # (1,H,W)\n",
    "        sim = torch.from_numpy(sim).unsqueeze(0).float()         # (1,H,W)\n",
    "\n",
    "        # --- Optional Data Augmentation ---\n",
    "        if self.augment:\n",
    "            if random.random() > 0.5:\n",
    "                img = torch.flip(img, dims=[2])\n",
    "                label = torch.flip(label, dims=[2])\n",
    "                imp = torch.flip(imp, dims=[2])\n",
    "                sim = torch.flip(sim, dims=[2])\n",
    "            if random.random() > 0.5:\n",
    "                img = torch.flip(img, dims=[1])\n",
    "                label = torch.flip(label, dims=[1])\n",
    "                imp = torch.flip(imp, dims=[1])\n",
    "                sim = torch.flip(sim, dims=[1])\n",
    "\n",
    "        # --- Apply transform if any ---\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return {\n",
    "            \"image\": img,\n",
    "            \"label\": label,\n",
    "            \"importance\": imp,\n",
    "            \"similarity\": sim,\n",
    "            \"name\": name\n",
    "        }\n",
    "\n",
    "# ============================================================\n",
    "# DATALOADER CREATION\n",
    "# ============================================================\n",
    "def get_dataloaders(batch_size=4, num_workers=2, augment=True):\n",
    "    dataset = PotsdamWeaklyDataset(DATA_DIR, augment=augment)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=num_workers, pin_memory=True)\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc5a7c87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:20:00.330507Z",
     "iopub.status.busy": "2025-11-02T18:20:00.330028Z",
     "iopub.status.idle": "2025-11-02T18:20:00.336594Z",
     "shell.execute_reply": "2025-11-02T18:20:00.336028Z"
    },
    "papermill": {
     "duration": 0.042549,
     "end_time": "2025-11-02T18:20:00.337612",
     "exception": false,
     "start_time": "2025-11-02T18:20:00.295063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # WEAKLY-SUPERVISED SEGMENTATION TRAINING (KAGGLE)\n",
    "# # ============================================================\n",
    "# # Author: thxshsv (Anirudh International LLC)\n",
    "# # Dataset: ISPRS Potsdam patched\n",
    "# # Input:  /kaggle/working/preprocessed_data/\n",
    "# # Output: /kaggle/working/models/unet_model.pth\n",
    "# # ============================================================\n",
    "\n",
    "# !pip install segmentation-models-pytorch --quiet\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch import nn, optim\n",
    "# from tqdm import tqdm\n",
    "# import segmentation_models_pytorch as smp\n",
    "# from pathlib import Path\n",
    "# import numpy as np\n",
    "\n",
    "# # ============================================================\n",
    "# # 1. CONFIG\n",
    "# # ============================================================\n",
    "# DATA_DIR = Path(\"/kaggle/working/preprocessed_data\")\n",
    "# MODEL_DIR = Path(\"/kaggle/working/models\")\n",
    "# MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# BATCH_SIZE = 8\n",
    "# EPOCHS = 20\n",
    "# LR = 1e-4\n",
    "# LAMBDA_IMP = 0.5\n",
    "# LAMBDA_SIM = 0.2\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(\"Running on:\", device)\n",
    "\n",
    "# # ============================================================\n",
    "# # 2. DATASET + DATALOADER (from previous step)\n",
    "# # ============================================================\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import random\n",
    "\n",
    "# class PotsdamWeaklyDataset(Dataset):\n",
    "#     def __init__(self, data_dir, augment=False):\n",
    "#         self.data_dir = Path(data_dir)\n",
    "#         self.img_paths = sorted((self.data_dir / \"images\").glob(\"*.npy\"))\n",
    "#         self.augment = augment\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.img_paths)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         name = self.img_paths[idx].stem\n",
    "#         img = np.load(self.data_dir / \"images\" / f\"{name}.npy\")\n",
    "#         lbl = np.load(self.data_dir / \"pseudo_labels\" / f\"{name}.npy\")\n",
    "#         imp = np.load(self.data_dir / \"importance_maps\" / f\"{name}.npy\")\n",
    "#         sim = np.load(self.data_dir / \"similarity_maps\" / f\"{name}.npy\")\n",
    "\n",
    "#         img = torch.from_numpy(img.transpose(2, 0, 1)).float()\n",
    "#         lbl = torch.from_numpy(lbl).unsqueeze(0).float()\n",
    "#         imp = torch.from_numpy(imp).unsqueeze(0).float()\n",
    "#         sim = torch.from_numpy(sim).unsqueeze(0).float()\n",
    "\n",
    "#         if self.augment:\n",
    "#             if random.random() > 0.5:\n",
    "#                 img = torch.flip(img, dims=[2])\n",
    "#                 lbl = torch.flip(lbl, dims=[2])\n",
    "#                 imp = torch.flip(imp, dims=[2])\n",
    "#                 sim = torch.flip(sim, dims=[2])\n",
    "#             if random.random() > 0.5:\n",
    "#                 img = torch.flip(img, dims=[1])\n",
    "#                 lbl = torch.flip(lbl, dims=[1])\n",
    "#                 imp = torch.flip(imp, dims=[1])\n",
    "#                 sim = torch.flip(sim, dims=[1])\n",
    "\n",
    "#         return {\"image\": img, \"label\": lbl, \"importance\": imp, \"similarity\": sim, \"name\": name}\n",
    "\n",
    "# def get_dataloaders(batch_size=8, augment=True):\n",
    "#     dataset = PotsdamWeaklyDataset(DATA_DIR, augment=augment)\n",
    "#     n = len(dataset)\n",
    "#     train_size = int(0.8 * n)\n",
    "#     val_size = n - train_size\n",
    "#     train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "#     train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "#     val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "#     return train_loader, val_loader\n",
    "\n",
    "# train_loader, val_loader = get_dataloaders(BATCH_SIZE)\n",
    "\n",
    "# # ============================================================\n",
    "# # 3. MODEL\n",
    "# # ============================================================\n",
    "# model = smp.Unet(\n",
    "#     encoder_name=\"resnet34\",\n",
    "#     encoder_weights=\"imagenet\",\n",
    "#     in_channels=3,\n",
    "#     classes=1\n",
    "# ).to(device)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# # ============================================================\n",
    "# # 4. METRICS\n",
    "# # ============================================================\n",
    "# def dice_coeff(pred, target, eps=1e-7):\n",
    "#     pred = (pred > 0.5).float()\n",
    "#     inter = (pred * target).sum()\n",
    "#     return (2 * inter + eps) / (pred.sum() + target.sum() + eps)\n",
    "\n",
    "# def iou_score(pred, target, eps=1e-7):\n",
    "#     pred = (pred > 0.5).float()\n",
    "#     inter = (pred * target).sum()\n",
    "#     union = pred.sum() + target.sum() - inter\n",
    "#     return (inter + eps) / (union + eps)\n",
    "\n",
    "# # ============================================================\n",
    "# # 5. TRAINING LOOP\n",
    "# # ============================================================\n",
    "# def compute_similarity_loss(preds_sig, sim):\n",
    "#     \"\"\"Spatial smoothness: encourages similar predictions in high-similarity regions.\"\"\"\n",
    "#     diff_x = (preds_sig[:, :, 1:, :] - preds_sig[:, :, :-1, :])**2\n",
    "#     diff_y = (preds_sig[:, :, :, 1:] - preds_sig[:, :, :, :-1])**2\n",
    "#     sim_x = sim[:, :, 1:, :]\n",
    "#     sim_y = sim[:, :, :, 1:]\n",
    "#     loss_x = (sim_x * diff_x).mean()\n",
    "#     loss_y = (sim_y * diff_y).mean()\n",
    "#     return loss_x + loss_y\n",
    "\n",
    "# def train_one_epoch(loader):\n",
    "#     model.train()\n",
    "#     total_loss, total_dice = 0, 0\n",
    "#     for batch in tqdm(loader, desc=\"Training\"):\n",
    "#         imgs = batch[\"image\"].to(device)\n",
    "#         labels = batch[\"label\"].to(device)\n",
    "#         imp = batch[\"importance\"].to(device)\n",
    "#         sim = batch[\"similarity\"].to(device)\n",
    "\n",
    "#         preds = model(imgs)\n",
    "#         preds_sig = torch.sigmoid(preds)\n",
    "\n",
    "#         # Losses\n",
    "#         sup_loss = F.binary_cross_entropy(preds_sig, labels)\n",
    "#         imp_loss = torch.mean(imp * (preds_sig - labels) ** 2)\n",
    "#         sim_loss = compute_similarity_loss(preds_sig, sim)\n",
    "#         loss = sup_loss + LAMBDA_IMP * imp_loss + LAMBDA_SIM * sim_loss\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "#         total_dice += dice_coeff(preds_sig, labels).item()\n",
    "#     return total_loss / len(loader), total_dice / len(loader)\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def validate_one_epoch(loader):\n",
    "#     model.eval()\n",
    "#     total_loss, total_dice, total_iou = 0, 0, 0\n",
    "#     for batch in tqdm(loader, desc=\"Validation\"):\n",
    "#         imgs = batch[\"image\"].to(device)\n",
    "#         labels = batch[\"label\"].to(device)\n",
    "#         imp = batch[\"importance\"].to(device)\n",
    "#         sim = batch[\"similarity\"].to(device)\n",
    "\n",
    "#         preds = model(imgs)\n",
    "#         preds_sig = torch.sigmoid(preds)\n",
    "\n",
    "#         sup_loss = F.binary_cross_entropy(preds_sig, labels)\n",
    "#         imp_loss = torch.mean(imp * (preds_sig - labels) ** 2)\n",
    "#         sim_loss = compute_similarity_loss(preds_sig, sim)\n",
    "#         loss = sup_loss + LAMBDA_IMP * imp_loss + LAMBDA_SIM * sim_loss\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "#         total_dice += dice_coeff(preds_sig, labels).item()\n",
    "#         total_iou += iou_score(preds_sig, labels).item()\n",
    "\n",
    "#     n = len(loader)\n",
    "#     return total_loss / n, total_dice / n, total_iou / n\n",
    "\n",
    "# # ============================================================\n",
    "# # 6. TRAINING\n",
    "# # ============================================================\n",
    "# best_dice = 0\n",
    "# for epoch in range(EPOCHS):\n",
    "#     train_loss, train_dice = train_one_epoch(train_loader)\n",
    "#     val_loss, val_dice, val_iou = validate_one_epoch(val_loader)\n",
    "\n",
    "#     print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "#     print(f\"Train Loss: {train_loss:.4f} | Train Dice: {train_dice:.4f}\")\n",
    "#     print(f\"Val   Loss: {val_loss:.4f} | Val Dice: {val_dice:.4f} | Val IoU: {val_iou:.4f}\")\n",
    "\n",
    "#     # Save best model\n",
    "#     if val_dice > best_dice:\n",
    "#         best_dice = val_dice\n",
    "#         torch.save(model.state_dict(), MODEL_DIR / \"unet_model.pth\")\n",
    "#         print(\"âœ… Model saved (best so far)\")\n",
    "\n",
    "# print(\"ğŸ¯ Training complete. Best Dice:\", best_dice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ede558c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:20:00.406969Z",
     "iopub.status.busy": "2025-11-02T18:20:00.406762Z"
    },
    "papermill": {
     "duration": 102.140687,
     "end_time": "2025-11-02T18:21:42.512281",
     "exception": false,
     "start_time": "2025-11-02T18:20:00.371594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440eb9396211406fa9a5671df77d7abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc1155faf5642b8bcc1b824ba3598b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/87.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e647af5a2df544efbaef5424b4eb5337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MAVIHybridNet initialized on CUDA\n",
      "ğŸ”’ ViT encoder frozen for warm-up phase.\n",
      "âœ… Optimizer and scheduler initialized.\n",
      "\n",
      "========== Epoch 1/50 ==========\n",
      "ğŸ”º Warmup LR adjusted to 1.67e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# MAVI-Hybrid: CNN + ViT + Attention Fusion for Weak Supervision\n",
    "# ============================================================\n",
    "# Dataset: ISPRS Potsdam (weakly-supervised)\n",
    "# Author: thxshsv (Anirudh International LLC)\n",
    "# ============================================================\n",
    "\n",
    "!pip install segmentation-models-pytorch timm --quiet\n",
    "\n",
    "import torch, torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.decoders.unet.decoder import UnetDecoder\n",
    "from timm.models.vision_transformer import vit_base_patch16_224\n",
    "from pathlib import Path\n",
    "import numpy as np, cv2, random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "DATA_DIR = Path(\"/kaggle/working/preprocessed_data\")\n",
    "UNLABELED_IMG_DIR = Path(\"/kaggle/input/private-data-1/patches/Images\")\n",
    "MODEL_DIR = Path(\"/kaggle/working/models\"); MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE   = 8\n",
    "EPOCHS       = 50\n",
    "LR           = 5e-5\n",
    "WARMUP_EPOCHS = 3\n",
    "LAMBDA_IMP   = 0.5\n",
    "LAMBDA_SIM   = 0.2\n",
    "REFINE_EVERY = 5\n",
    "PATIENCE     = 7\n",
    "UNFREEZE_EPOCH = 5  # unfreeze ViT after 5 epochs\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Running on:\", device)\n",
    "\n",
    "# ============================================================\n",
    "# DATASET\n",
    "# ============================================================\n",
    "class PotsdamWeaklyDataset(Dataset):\n",
    "    def __init__(self, data_dir, augment=False):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.img_paths = sorted((self.data_dir / \"images\").glob(\"*.npy\"))\n",
    "        self.augment = augment\n",
    "    def __len__(self): return len(self.img_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.img_paths[idx].stem\n",
    "        img = np.load(self.data_dir / \"images\" / f\"{name}.npy\")\n",
    "        lbl = np.load(self.data_dir / \"pseudo_labels\" / f\"{name}.npy\")\n",
    "        imp = np.load(self.data_dir / \"importance_maps\" / f\"{name}.npy\")\n",
    "        sim = np.load(self.data_dir / \"similarity_maps\" / f\"{name}.npy\")\n",
    "\n",
    "        img = torch.from_numpy(img.transpose(2,0,1)).float()\n",
    "        lbl = torch.from_numpy(lbl).unsqueeze(0).float()\n",
    "        imp = torch.from_numpy(imp).unsqueeze(0).float()\n",
    "        sim = torch.from_numpy(sim).unsqueeze(0).float()\n",
    "\n",
    "        if self.augment:\n",
    "            if random.random() > 0.5:\n",
    "                img,lbl,imp,sim = [torch.flip(t,dims=[2]) for t in [img,lbl,imp,sim]]\n",
    "            if random.random() > 0.5:\n",
    "                img,lbl,imp,sim = [torch.flip(t,dims=[1]) for t in [img,lbl,imp,sim]]\n",
    "\n",
    "        return {\"image\": img, \"label\": lbl, \"importance\": imp, \"similarity\": sim}\n",
    "\n",
    "def get_dataloaders(batch_size=BATCH_SIZE, augment=True):\n",
    "    ds = PotsdamWeaklyDataset(DATA_DIR, augment)\n",
    "    if len(ds)==0: raise ValueError(\"No preprocessed .npy found\")\n",
    "    tr_sz = int(0.8*len(ds)); val_sz = len(ds)-tr_sz\n",
    "    tr,val = torch.utils.data.random_split(ds,[tr_sz,val_sz])\n",
    "    return (DataLoader(tr,batch_size,True,num_workers=2),\n",
    "            DataLoader(val,batch_size,False,num_workers=2))\n",
    "\n",
    "train_loader,val_loader=get_dataloaders()\n",
    "\n",
    "# ============================================================\n",
    "# ATTENTION FUSION BLOCK\n",
    "# ============================================================\n",
    "class AttentionFusionBlock(nn.Module):\n",
    "    def __init__(self, cnn_channels=512, vit_dim=768):\n",
    "        super().__init__()\n",
    "        self.query = nn.Conv2d(cnn_channels, vit_dim, 1)\n",
    "        self.key   = nn.Linear(vit_dim, vit_dim)\n",
    "        self.value = nn.Linear(vit_dim, vit_dim)\n",
    "        self.proj  = nn.Conv2d(vit_dim, cnn_channels, 1)\n",
    "        self.scale = vit_dim ** -0.5\n",
    "    def forward(self, cnn_feats, vit_tokens):\n",
    "        B,C,H,W = cnn_feats.shape\n",
    "        q = self.query(cnn_feats).flatten(2).transpose(1,2)\n",
    "        k = self.key(vit_tokens); v = self.value(vit_tokens)\n",
    "        attn = torch.softmax((q @ k.transpose(-2,-1))*self.scale, dim=-1)\n",
    "        fused = (attn @ v).transpose(1,2).reshape(B,-1,H,W)\n",
    "        return self.proj(fused) + cnn_feats\n",
    "\n",
    "# ============================================================\n",
    "# MAVI-HYBRID MODEL (Fixed SMP Compatibility)\n",
    "# ============================================================\n",
    "# --- keep your existing imports ---\n",
    "# import segmentation_models_pytorch as smp\n",
    "# from timm.models.vision_transformer import vit_base_patch16_224\n",
    "# (and your AttentionFusionBlock)\n",
    "\n",
    "class MAVIHybridNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Build a full SMP Unet (it creates encoder+decoder wired correctly)\n",
    "        self.base_unet = smp.Unet(\n",
    "            encoder_name=\"resnet34\",\n",
    "            encoder_weights=\"imagenet\",\n",
    "            in_channels=3,\n",
    "            classes=1\n",
    "        )\n",
    "        # ViT for global tokens\n",
    "        self.vit = vit_base_patch16_224(pretrained=True)\n",
    "        # Our fusion acts on the deepest encoder feature\n",
    "        # ResNet34 deepest feature is 512 ch, ViT-B/16 has 768-d tokens\n",
    "        self.fusion = AttentionFusionBlock(cnn_channels=512, vit_dim=768)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1) Encoder features from SMP (list of 5 tensors, shallow -> deep)\n",
    "        feats = self.base_unet.encoder(x)   # [f0(64), f1(64), f2(128), f3(256), f4(512)]\n",
    "\n",
    "        # 2) ViT tokens (use resized input for ViT)\n",
    "        vit_tokens = self.vit.forward_features(F.interpolate(x, size=(224, 224)))\n",
    "\n",
    "        # 3) Fuse on the deepest feature (keep same 512 ch, same spatial size)\n",
    "        fused_deep = self.fusion(feats[-1], vit_tokens)\n",
    "\n",
    "        # 4) Replace deepest feature with fused one\n",
    "        feats[-1] = fused_deep\n",
    "\n",
    "        # 5) Decode using SMPâ€™s own decoder (guaranteed channel correctness)\n",
    "        dec = self.base_unet.decoder(feats)\n",
    "\n",
    "        # 6) Final segmentation head\n",
    "        out = self.base_unet.segmentation_head(dec)\n",
    "        return out\n",
    "\n",
    "# ============================================================\n",
    "# MODEL INIT + OPTIMIZER + SCHEDULER\n",
    "# ============================================================\n",
    "model = MAVIHybridNet().to(device)\n",
    "print(f\"âœ… MAVIHybridNet initialized on {device.upper()}\")\n",
    "\n",
    "# Freeze ViT for stability\n",
    "for p in model.vit.parameters():\n",
    "    p.requires_grad = False\n",
    "print(\"ğŸ”’ ViT encoder frozen for warm-up phase.\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n",
    "print(\"âœ… Optimizer and scheduler initialized.\")\n",
    "\n",
    "# ============================================================\n",
    "# WARMUP LR\n",
    "# ============================================================\n",
    "def warmup_lr(optimizer, epoch, warmup_epochs=WARMUP_EPOCHS, base_lr=LR):\n",
    "    if epoch < warmup_epochs:\n",
    "        lr_scale = (epoch + 1) / warmup_epochs\n",
    "        for pg in optimizer.param_groups:\n",
    "            pg[\"lr\"] = base_lr * lr_scale\n",
    "        print(f\"ğŸ”º Warmup LR adjusted to {base_lr * lr_scale:.2e}\")\n",
    "\n",
    "# ============================================================\n",
    "# METRICS / LOSSES\n",
    "# ============================================================\n",
    "def dice(pred,gt,eps=1e-7):\n",
    "    p=(pred>0.5).float(); inter=(p*gt).sum()\n",
    "    return (2*inter+eps)/(p.sum()+gt.sum()+eps)\n",
    "def iou(pred,gt,eps=1e-7):\n",
    "    p=(pred>0.5).float(); inter=(p*gt).sum()\n",
    "    return (inter+eps)/(p.sum()+gt.sum()-inter+eps)\n",
    "def sim_loss(pred,sim):\n",
    "    dx=(pred[:,:,1:,:]-pred[:,:,:-1,:])**2\n",
    "    dy=(pred[:,:,:,1:]-pred[:,:,:,:-1])**2\n",
    "    return (sim[:,:,1:,:]*dx).mean()+(sim[:,:,:,1:]*dy).mean()\n",
    "\n",
    "# ============================================================\n",
    "# PSEUDO-LABEL REFINEMENT\n",
    "# ============================================================\n",
    "def generate_pseudo_labels(model, img_dir=UNLABELED_IMG_DIR, save_dir=DATA_DIR/\"pseudo_labels\"):\n",
    "    model.eval(); imgs=sorted(img_dir.glob(\"*.tif\"))\n",
    "    print(f\"ğŸŒ€ Regenerating pseudo-labels for {len(imgs)} images...\")\n",
    "    for p in tqdm(imgs):\n",
    "        name=p.stem; img=cv2.imread(str(p))\n",
    "        if img is None: continue\n",
    "        img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB); img=cv2.resize(img,(512,512))\n",
    "        t=torch.from_numpy(img.transpose(2,0,1)/255.).unsqueeze(0).float().to(device)\n",
    "        with torch.no_grad(): pred=torch.sigmoid(model(t)).cpu().squeeze().numpy()\n",
    "        pred=cv2.medianBlur((pred*255).astype(np.uint8),3).astype(np.float32)/255.\n",
    "        np.save(save_dir/f\"{name}.npy\",pred)\n",
    "    print(\"âœ… Pseudo-label update done!\")\n",
    "\n",
    "# ============================================================\n",
    "# TRAIN / VALIDATE\n",
    "# ============================================================\n",
    "def run_epoch(loader,train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    totL=totD=totI=0\n",
    "    for b in tqdm(loader,desc=\"Train\" if train else \"Val\"):\n",
    "        x=b[\"image\"].to(device); y=b[\"label\"].to(device)\n",
    "        imp=b[\"importance\"].to(device); sim=b[\"similarity\"].to(device)\n",
    "        with torch.set_grad_enabled(train):\n",
    "            p=torch.sigmoid(model(x))\n",
    "            sup=F.binary_cross_entropy(p,y)\n",
    "            impL=(imp*(p-y)**2).mean(); sL=sim_loss(p,sim)\n",
    "            loss=sup+LAMBDA_IMP*impL+LAMBDA_SIM*sL\n",
    "            if train: optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        totL+=loss.item(); totD+=dice(p,y).item(); totI+=iou(p,y).item()\n",
    "    n=len(loader)\n",
    "    return totL/n,totD/n,totI/n\n",
    "\n",
    "# ============================================================\n",
    "# MAIN LOOP\n",
    "# ============================================================\n",
    "best_dice,epochs_no_improve=0,0\n",
    "for ep in range(EPOCHS):\n",
    "    print(f\"\\n========== Epoch {ep+1}/{EPOCHS} ==========\")\n",
    "    warmup_lr(optimizer, ep)\n",
    "\n",
    "    # Gradual unfreezing\n",
    "    if ep == UNFREEZE_EPOCH:\n",
    "        print(\"ğŸ”“ Unfreezing ViT encoder for fine-tuning...\")\n",
    "        for p in model.vit.parameters(): p.requires_grad = True\n",
    "\n",
    "    tr_l,tr_d,_=run_epoch(train_loader,True)\n",
    "    val_l,val_d,val_i=run_epoch(val_loader,False)\n",
    "    scheduler.step()\n",
    "    print(f\"Train L:{tr_l:.4f}|D:{tr_d:.4f}  Val L:{val_l:.4f}|D:{val_d:.4f}|IoU:{val_i:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_d>best_dice:\n",
    "        best_dice=val_d; epochs_no_improve=0\n",
    "        torch.save(model.state_dict(), MODEL_DIR/\"mavi_hybrid_best.pth\")\n",
    "        print(\"âœ… Saved best model.\")\n",
    "    else:\n",
    "        epochs_no_improve+=1\n",
    "        if epochs_no_improve>=PATIENCE:\n",
    "            print(\"â¹ Early stopping triggered.\"); break\n",
    "\n",
    "    # Pseudo-label refinement\n",
    "    if (ep+1)%REFINE_EVERY==0:\n",
    "        print(f\"ğŸ” Refining pseudo-labels after epoch {ep+1}\")\n",
    "        generate_pseudo_labels(model)\n",
    "        train_loader,val_loader=get_dataloaders()\n",
    "\n",
    "print(f\"ğŸ¯ Training complete. Best Dice={best_dice:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efceca7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T17:02:56.432655Z",
     "iopub.status.busy": "2025-11-02T17:02:56.432006Z",
     "iopub.status.idle": "2025-11-02T17:04:55.481493Z",
     "shell.execute_reply": "2025-11-02T17:04:55.480508Z",
     "shell.execute_reply.started": "2025-11-02T17:02:56.432630Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# EVALUATION (robust saving + PR/AP + threshold sweep)\n",
    "# ==============================================\n",
    "import numpy as np, torch, shutil, cv2, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "METRICS_DIR = Path(\"/kaggle/working/metrics\"); METRICS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PREDS_DIR   = Path(\"/kaggle/working/preds\")\n",
    "\n",
    "def _safe_save(path, arr, mode=\"png\"):\n",
    "    \"\"\"Save probability or mask with minimal IO issues.\"\"\"\n",
    "    try:\n",
    "        if mode == \"png\":\n",
    "            a = np.clip(arr, 0, 1) * 255.0\n",
    "            a = a.astype(np.uint8)\n",
    "            return bool(cv2.imwrite(str(path.with_suffix(\".png\")), a))\n",
    "        elif mode == \"npz\":\n",
    "            np.savez_compressed(path.with_suffix(\".npz\"), prob=arr.astype(np.float16))\n",
    "            return True\n",
    "        else:\n",
    "            np.save(path.with_suffix(\".npy\"), arr); return True\n",
    "    except OSError as e:\n",
    "        print(f\"âš ï¸ Save failed at {path.name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def dice_score(y_true_bin, y_pred_bin, eps=1e-7):\n",
    "    inter = (y_true_bin & y_pred_bin).sum()\n",
    "    return (2*inter + eps) / (y_true_bin.sum() + y_pred_bin.sum() + eps)\n",
    "\n",
    "def iou_score(y_true_bin, y_pred_bin, eps=1e-7):\n",
    "    inter = (y_true_bin & y_pred_bin).sum()\n",
    "    union = y_true_bin.sum() + y_pred_bin.sum() - inter\n",
    "    return (inter + eps) / (union + eps)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(\n",
    "    model,\n",
    "    loader,\n",
    "    device,\n",
    "    default_thresh=0.5,\n",
    "    max_save=120,         # limit disk writes; set 0 to skip saving\n",
    "    save_mode=\"png\",      # 'png'|'npz'|'npy'\n",
    "    save_masks=True\n",
    "):\n",
    "    # clean preds dir\n",
    "    if PREDS_DIR.exists():\n",
    "        shutil.rmtree(PREDS_DIR)\n",
    "    (PREDS_DIR/\"probs\").mkdir(parents=True, exist_ok=True)\n",
    "    (PREDS_DIR/\"masks\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_gts   = []\n",
    "    save_idx = 0\n",
    "    saving_ok = True\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Eval/Val\"):\n",
    "        x = batch[\"image\"].to(device)\n",
    "        y = batch[\"label\"].cpu().numpy()          # (B,1,H,W) weak labels in [0,1]\n",
    "        p = torch.sigmoid(model(x)).cpu().numpy()  # (B,1,H,W)\n",
    "\n",
    "        for i in range(p.shape[0]):\n",
    "            prob = p[i,0]\n",
    "            gt   = y[i,0]\n",
    "            all_probs.append(prob.ravel())\n",
    "            all_gts.append(gt.ravel())\n",
    "\n",
    "            if saving_ok and save_idx < max_save:\n",
    "                ok1 = _safe_save((PREDS_DIR/\"probs\"/f\"val_{save_idx:06d}\"), prob, mode=save_mode)\n",
    "                ok2 = True\n",
    "                if save_masks:\n",
    "                    mask = (prob >= default_thresh).astype(np.uint8) * 255\n",
    "                    ok2 = bool(cv2.imwrite(str((PREDS_DIR/\"masks\"/f\"val_{save_idx:06d}\").with_suffix(\".png\")), mask))\n",
    "                if not (ok1 and ok2):\n",
    "                    print(\"ğŸ’¾ Disk pressure: switching to metrics-only (no further saves).\")\n",
    "                    saving_ok = False\n",
    "            save_idx += 1\n",
    "\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    all_gts   = np.concatenate(all_gts,   axis=0)\n",
    "\n",
    "    # ---- Binzarize GT for PR/AP ----\n",
    "    gt_bin_vec = (all_gts >= 0.5).astype(np.uint8)\n",
    "    m = np.isfinite(all_probs) & np.isfinite(gt_bin_vec)\n",
    "    all_probs_c = all_probs[m]\n",
    "    gt_bin_vec_c = gt_bin_vec[m]\n",
    "\n",
    "    # PR + AP\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(gt_bin_vec_c, all_probs_c)\n",
    "    ap = average_precision_score(gt_bin_vec_c, all_probs_c)\n",
    "\n",
    "    # Threshold sweep (Dice/IoU)\n",
    "    sweep = np.linspace(0.05, 0.95, 19)\n",
    "    dice_list, iou_list = [], []\n",
    "    gt_bool = gt_bin_vec_c.astype(bool)\n",
    "    for t in sweep:\n",
    "        pred_bin = (all_probs_c >= t)\n",
    "        dice_list.append(dice_score(gt_bool, pred_bin))\n",
    "        iou_list.append(iou_score(gt_bool, pred_bin))\n",
    "\n",
    "    best_idx = int(np.argmax(dice_list))\n",
    "    best_t   = float(sweep[best_idx])\n",
    "    best_d   = float(dice_list[best_idx])\n",
    "    best_i   = float(iou_list[best_idx])\n",
    "\n",
    "    # --- plots ---\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.plot(recall, precision)\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR curve (AP={ap:.4f})\")\n",
    "    plt.grid(True, alpha=0.3); plt.tight_layout()\n",
    "    plt.savefig(METRICS_DIR/\"pr_curve.png\", dpi=150); plt.close()\n",
    "\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.plot(sweep, dice_list, label=\"Dice\")\n",
    "    plt.plot(sweep, iou_list,  label=\"IoU\", alpha=0.8)\n",
    "    plt.axvline(best_t, ls=\"--\", color=\"k\", label=f\"Best t={best_t:.2f}\")\n",
    "    plt.xlabel(\"Threshold\"); plt.ylabel(\"Score\"); plt.title(\"Threshold sweep\")\n",
    "    plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout()\n",
    "    plt.savefig(METRICS_DIR/\"dice_iou_vs_threshold.png\", dpi=150); plt.close()\n",
    "\n",
    "    with open(METRICS_DIR/\"summary.txt\",\"w\") as f:\n",
    "        f.write(f\"Average Precision (AP): {ap:.6f}\\n\")\n",
    "        f.write(f\"Best threshold (Dice): {best_t:.4f}\\n\")\n",
    "        f.write(f\"Best Dice: {best_d:.6f}\\n\")\n",
    "        f.write(f\"Best IoU:  {best_i:.6f}\\n\")\n",
    "        f.write(f\"Default threshold saved masks: {default_thresh:.2f}\\n\")\n",
    "        f.write(f\"Files saved (max_save): {min(save_idx, max_save)}; save_mode={save_mode}\\n\")\n",
    "\n",
    "    print(\"============================================\")\n",
    "    print(f\"AP: {ap:.4f}\")\n",
    "    print(f\"Best t (Dice): {best_t:.3f} | Dice: {best_d:.4f} | IoU: {best_i:.4f}\")\n",
    "    print(f\"Saved plots to: {METRICS_DIR}\")\n",
    "    print(f\"Saved some predictions to: {PREDS_DIR} (mode={save_mode}, max_save={max_save})\")\n",
    "    print(\"============================================\")\n",
    "\n",
    "    return {\n",
    "        \"ap\": ap, \"best_threshold\": best_t,\n",
    "        \"best_dice\": best_d, \"best_iou\": best_i,\n",
    "        \"precision\": precision, \"recall\": recall,\n",
    "        \"sweep\": sweep, \"dice_list\": dice_list, \"iou_list\": iou_list\n",
    "    }\n",
    "\n",
    "# ---- Run evaluation ----\n",
    "eval_results = evaluate_model(\n",
    "    model, val_loader, device,\n",
    "    default_thresh=0.5,\n",
    "    max_save=120,         # reduce if IO errors; set 0 to skip saving\n",
    "    save_mode=\"png\",      # 'png' is small; 'npz' also good\n",
    "    save_masks=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24261ed6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T17:11:40.052352Z",
     "iopub.status.busy": "2025-11-02T17:11:40.051499Z",
     "iopub.status.idle": "2025-11-02T17:11:44.194367Z",
     "shell.execute_reply": "2025-11-02T17:11:44.193146Z",
     "shell.execute_reply.started": "2025-11-02T17:11:40.052324Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# VIS: Original Image + Predicted Mask (Overlays)\n",
    "# ==============================================\n",
    "import os, cv2, numpy as np, torch\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "OVERLAY_DIR = Path(\"/kaggle/working/overlays\"); OVERLAY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Use best threshold from eval (fallback to 0.55 if not present)\n",
    "BEST_T = float(eval_results[\"best_threshold\"]) if \"eval_results\" in globals() else 0.55\n",
    "print(f\"Using threshold = {BEST_T:.2f} for overlays\")\n",
    "\n",
    "def to_numpy_img(t):  # (3,H,W) torch -> (H,W,3) np float [0,1]\n",
    "    a = t.detach().cpu().numpy()\n",
    "    a = np.transpose(a, (1,2,0))\n",
    "    return np.clip(a, 0, 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def save_overlays(model, loader, out_dir=OVERLAY_DIR, max_batches=5):\n",
    "    model.eval()\n",
    "    saved = 0\n",
    "    for b, batch in enumerate(tqdm(loader, desc=\"Overlays\")):\n",
    "        if b >= max_batches: break\n",
    "        x = batch[\"image\"].to(device)              # (B,3,H,W)\n",
    "        probs = torch.sigmoid(model(x)).cpu().numpy()  # (B,1,H,W)\n",
    "\n",
    "        for i in range(x.size(0)):\n",
    "            img = (to_numpy_img(x[i]) * 255).astype(np.uint8)      # RGB uint8\n",
    "            prob = probs[i,0]                                      # (H,W) float\n",
    "            mask = (prob >= BEST_T).astype(np.uint8)               # (H,W) {0,1}\n",
    "\n",
    "            # Colorize mask (green) and blend on top of image\n",
    "            color = np.zeros_like(img)\n",
    "            color[:,:,1] = (mask * 255)  # put mask in G channel\n",
    "            overlay = cv2.addWeighted(img, 1.0, color, 0.45, 0)    # 45% mask\n",
    "\n",
    "            # Also add a probability heatmap side-by-side (optional)\n",
    "            heat = (prob * 255).astype(np.uint8)\n",
    "            heat = cv2.applyColorMap(heat, cv2.COLORMAP_TURBO)     # COLORMAP_JET also ok\n",
    "            heat = cv2.cvtColor(heat, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Stack: [original | overlay | heatmap]\n",
    "            grid = np.concatenate([img, overlay, heat], axis=1)\n",
    "            cv2.imwrite(str(out_dir / f\"val_{saved:06d}.png\"), cv2.cvtColor(grid, cv2.COLOR_RGB2BGR))\n",
    "            saved += 1\n",
    "    print(f\"Saved {saved} overlay images to: {out_dir}\")\n",
    "\n",
    "# Run it\n",
    "save_overlays(model, val_loader, OVERLAY_DIR, max_batches=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299ae036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T17:32:43.156030Z",
     "iopub.status.busy": "2025-11-02T17:32:43.155416Z",
     "iopub.status.idle": "2025-11-02T17:32:45.792657Z",
     "shell.execute_reply": "2025-11-02T17:32:45.791870Z",
     "shell.execute_reply.started": "2025-11-02T17:32:43.156005Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# ROI Compression on RAW .tif with better strategies (metrics-only)\n",
    "# Baselines: JPEG q=90 and WebP q=80\n",
    "# Strategies: \n",
    "#   - binary_keep_png  (mask bg to 0, save PNG)\n",
    "#   - soft_keep_webp   (blur+dim bg, WebP q=60)\n",
    "#   - roi_only_png     (ONLY tight ROI crop as PNG)\n",
    "#   - roi_crop_ctx     (ROI PNG crop + context WebP q=35)\n",
    "# ==============================================\n",
    "import cv2, numpy as np, torch, glob\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "RAW_IMG_DIR = Path(\"/kaggle/input/private-data-1/patches/Images\")\n",
    "SAVE_FEW_SAMPLES = 0  # set >0 to save previews (uses /kaggle/working, mind disk)\n",
    "SAMPLES_OUT_DIR = Path(\"/kaggle/working/roi_outputs_better\"); SAMPLES_OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "BEST_T = float(eval_results[\"best_threshold\"]) if \"eval_results\" in globals() else 0.55\n",
    "print(f\"Using threshold for ROI = {BEST_T:.2f}\")\n",
    "\n",
    "def _enc_jpg(rgb, q): \n",
    "    ok, enc = cv2.imencode(\".jpg\", cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR), [cv2.IMWRITE_JPEG_QUALITY, int(q)])\n",
    "    if not ok: raise RuntimeError(\"JPEG encode failed\"); \n",
    "    return enc.tobytes()\n",
    "\n",
    "def _enc_png(rgb, level=3):\n",
    "    ok, enc = cv2.imencode(\".png\", cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR), [cv2.IMWRITE_PNG_COMPRESSION, int(level)])\n",
    "    if not ok: raise RuntimeError(\"PNG encode failed\")\n",
    "    return enc.tobytes()\n",
    "\n",
    "def _enc_webp(rgb, q=80):\n",
    "    ok, enc = cv2.imencode(\".webp\", cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR), [cv2.IMWRITE_WEBP_QUALITY, int(q)])\n",
    "    if not ok: raise RuntimeError(\"WEBP encode failed\")\n",
    "    return enc.tobytes()\n",
    "\n",
    "def _tight_bbox(mask):\n",
    "    ys, xs = np.where(mask > 0)\n",
    "    if len(xs) == 0: return None\n",
    "    return xs.min(), ys.min(), xs.max(), ys.max()\n",
    "\n",
    "def compress_strategies(rgb, mask01):\n",
    "    \"\"\"Return dict of byte counts for different strategies, plus (optional) preview images.\"\"\"\n",
    "    H, W = mask01.shape\n",
    "    mask3 = np.dstack([mask01]*3).astype(np.uint8)\n",
    "\n",
    "    # Baselines\n",
    "    base_jpg90 = len(_enc_jpg(rgb, q=90))\n",
    "    base_webp80 = len(_enc_webp(rgb, q=80))\n",
    "\n",
    "    # Strategy 1: binary_keep_png (zero bg) -> PNG\n",
    "    img_bin = rgb.copy()\n",
    "    img_bin[mask3 == 0] = 0\n",
    "    s1 = len(_enc_png(img_bin, level=3))\n",
    "\n",
    "    # Strategy 2: soft_keep_webp (blur+dim bg) -> WebP q=60\n",
    "    bg = cv2.GaussianBlur(rgb, (15,15), 0)\n",
    "    bg = (bg.astype(np.float32) * 0.35).astype(np.uint8)\n",
    "    soft = (mask3 * rgb + (1 - mask3) * bg).astype(np.uint8)\n",
    "    s2 = len(_enc_webp(soft, q=60))\n",
    "\n",
    "    # Strategy 3: roi_only_png (no context)\n",
    "    bb = _tight_bbox(mask01)\n",
    "    if bb is None:\n",
    "        s3 = len(_enc_webp(rgb, q=35))  # no ROI; send a tiny context\n",
    "    else:\n",
    "        x1,y1,x2,y2 = bb\n",
    "        pad = max(4, int(0.01*max(H,W)))\n",
    "        x1 = max(0, x1 - pad); y1 = max(0, y1 - pad)\n",
    "        x2 = min(W-1, x2 + pad); y2 = min(H-1, y2 + pad)\n",
    "        crop = rgb[y1:y2+1, x1:x2+1]\n",
    "        s3 = len(_enc_png(crop, level=3))\n",
    "\n",
    "    # Strategy 4: roi_crop_ctx (ROI PNG + tiny context WebP q=35)\n",
    "    if bb is None:\n",
    "        s4 = len(_enc_webp(rgb, q=35))\n",
    "    else:\n",
    "        crop = rgb[y1:y2+1, x1:x2+1]\n",
    "        crop_png = len(_enc_png(crop, level=3))\n",
    "        ctx_webp = len(_enc_webp(rgb, q=35))\n",
    "        s4 = crop_png + ctx_webp\n",
    "\n",
    "    sizes = {\n",
    "        \"baseline_jpg90\": base_jpg90,\n",
    "        \"baseline_webp80\": base_webp80,\n",
    "        \"binary_keep_png\": s1,\n",
    "        \"soft_keep_webp\": s2,\n",
    "        \"roi_only_png\": s3,\n",
    "        \"roi_crop_ctx\": s4,\n",
    "    }\n",
    "    previews = {\n",
    "        \"bin\": img_bin,\n",
    "        \"soft\": soft,\n",
    "        \"crop\": crop if bb is not None else rgb,\n",
    "    }\n",
    "    return sizes, previews\n",
    "# Minimal diagnostic runner: executes the ROI bench and RETURNS a summary dict\n",
    "import numpy as np\n",
    "\n",
    "def run_roi_bench_quick(model, max_images=20, threshold=0.55):\n",
    "    # Ensure the improved function exists; re-import np inside for safety\n",
    "    @torch.no_grad()\n",
    "    def _runner():\n",
    "        import numpy as np, glob, cv2, torch\n",
    "        from pathlib import Path\n",
    "        paths = sorted(glob.glob(str(Path(\"/kaggle/input/private-data-1/patches/Images\") / \"*.tif\")))\n",
    "        if len(paths) == 0:\n",
    "            print(\"âš ï¸ No .tif images found\"); \n",
    "            return None\n",
    "\n",
    "        agg = {k: [] for k in [\"baseline_jpg90\",\"baseline_webp80\",\"binary_keep_png\",\"soft_keep_webp\",\"roi_only_png\",\"roi_crop_ctx\"]}\n",
    "        for p in paths[:max_images]:\n",
    "            im_bgr = cv2.imread(p)\n",
    "            if im_bgr is None: \n",
    "                continue\n",
    "            im_rgb = cv2.cvtColor(im_bgr, cv2.COLOR_BGR2RGB)\n",
    "            H, W = im_rgb.shape[:2]\n",
    "\n",
    "            im_resz = cv2.resize(im_rgb, (512,512))\n",
    "            t = torch.from_numpy((im_resz/255.0).transpose(2,0,1)).unsqueeze(0).float().to(device)\n",
    "            prob = torch.sigmoid(model(t)).cpu().numpy()[0,0]\n",
    "            mask = cv2.resize((prob >= threshold).astype(np.uint8), (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            sizes, _ = compress_strategies(im_rgb, mask)\n",
    "            for k,v in sizes.items():\n",
    "                agg[k].append(v)\n",
    "\n",
    "        if len(agg[\"baseline_jpg90\"]) == 0:\n",
    "            print(\"âš ï¸ No images processed.\"); \n",
    "            return None\n",
    "\n",
    "        # Build a clean summary dict (averages + % savings)\n",
    "        b_jpg = np.array(agg[\"baseline_jpg90\"], dtype=np.float64)\n",
    "        b_wbp = np.array(agg[\"baseline_webp80\"], dtype=np.float64)\n",
    "        summary = {\"vs_jpeg90\": {}, \"vs_webp80\": {}}\n",
    "        for m in [\"binary_keep_png\",\"soft_keep_webp\",\"roi_only_png\",\"roi_crop_ctx\"]:\n",
    "            out = np.array(agg[m], dtype=np.float64)\n",
    "            summ = {\n",
    "                \"mean_orig_kb\": float(b_jpg.mean()/1024),\n",
    "                \"mean_out_kb\":  float(out.mean()/1024),\n",
    "                \"saving_pct\":   float((1 - out.mean()/b_jpg.mean())*100.0)\n",
    "            }\n",
    "            summary[\"vs_jpeg90\"][m] = summ\n",
    "            summ2 = {\n",
    "                \"mean_orig_kb\": float(b_wbp.mean()/1024),\n",
    "                \"mean_out_kb\":  float(out.mean()/1024),\n",
    "                \"saving_pct\":   float((1 - out.mean()/b_wbp.mean())*100.0)\n",
    "            }\n",
    "            summary[\"vs_webp80\"][m] = summ2\n",
    "        return summary\n",
    "\n",
    "    res = _runner()\n",
    "    if res is None:\n",
    "        print(\"No results produced.\")\n",
    "    else:\n",
    "        # pretty print\n",
    "        print(\"\\n=== Savings vs JPEG q=90 baseline ===\")\n",
    "        for k,v in res[\"vs_jpeg90\"].items():\n",
    "            print(f\"{k:15s} | mean orig: {v['mean_orig_kb']:.1f} KB | mean out: {v['mean_out_kb']:.1f} KB | saving: {v['saving_pct']:.1f}%\")\n",
    "        print(\"\\n=== Savings vs WebP q=80 baseline ===\")\n",
    "        for k,v in res[\"vs_webp80\"].items():\n",
    "            print(f\"{k:15s} | mean orig: {v['mean_orig_kb']:.1f} KB | mean out: {v['mean_out_kb']:.1f} KB | saving: {v['saving_pct']:.1f}%\")\n",
    "    return res\n",
    "\n",
    "# Call it (uses the same BEST_T from your eval)\n",
    "_ = run_roi_bench_quick(model, max_images=20, threshold=BEST_T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cba0f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T17:36:00.203134Z",
     "iopub.status.busy": "2025-11-02T17:36:00.202018Z",
     "iopub.status.idle": "2025-11-02T17:36:14.540071Z",
     "shell.execute_reply": "2025-11-02T17:36:14.539195Z",
     "shell.execute_reply.started": "2025-11-02T17:36:00.203057Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# Adaptive ROI encoder (metrics-only, space-safe)\n",
    "# ==============================================\n",
    "import cv2, numpy as np, torch, glob\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "RAW_IMG_DIR = Path(\"/kaggle/input/private-data-1/patches/Images\")\n",
    "SAVE_FEW_SAMPLES = 0\n",
    "SAMPLES_OUT_DIR = Path(\"/kaggle/working/roi_adaptive_samples\"); SAMPLES_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BEST_T = float(eval_results[\"best_threshold\"]) if \"eval_results\" in globals() else 0.55\n",
    "print(f\"Using threshold for ROI = {BEST_T:.2f}\")\n",
    "\n",
    "def _enc_jpg(rgb, q): \n",
    "    ok, enc = cv2.imencode(\".jpg\", cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR), [cv2.IMWRITE_JPEG_QUALITY, int(q)])\n",
    "    return enc.tobytes() if ok else None\n",
    "\n",
    "def _enc_png(rgb, level=3):\n",
    "    ok, enc = cv2.imencode(\".png\", cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR), [cv2.IMWRITE_PNG_COMPRESSION, int(level)])\n",
    "    return enc.tobytes() if ok else None\n",
    "\n",
    "def _enc_webp(rgb, q=80):\n",
    "    ok, enc = cv2.imencode(\".webp\", cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR), [cv2.IMWRITE_WEBP_QUALITY, int(q)])\n",
    "    return enc.tobytes() if ok else None\n",
    "\n",
    "def _tight_bbox(mask):\n",
    "    ys, xs = np.where(mask > 0)\n",
    "    if len(xs) == 0: return None\n",
    "    return xs.min(), ys.min(), xs.max(), ys.max()\n",
    "\n",
    "def _edge_density(gray_uint8):\n",
    "    gx = cv2.Sobel(gray_uint8, cv2.CV_32F, 1, 0, ksize=3)\n",
    "    gy = cv2.Sobel(gray_uint8, cv2.CV_32F, 0, 1, ksize=3)\n",
    "    mag = np.sqrt(gx*gx + gy*gy)\n",
    "    return float((mag > 25).mean())  # fraction of edge pixels\n",
    "\n",
    "def _overlay(rgb, mask):\n",
    "    color = np.zeros_like(rgb); color[:,:,1] = (mask*255)\n",
    "    return cv2.addWeighted(rgb, 1.0, color, 0.45, 0)\n",
    "\n",
    "def _binary_keep_png(rgb, mask01):\n",
    "    m3 = np.dstack([mask01]*3).astype(np.uint8)\n",
    "    out = rgb.copy(); out[m3==0] = 0\n",
    "    return _enc_png(out, level=3), out\n",
    "\n",
    "def _soft_keep_webp(rgb, mask01, q=60):\n",
    "    m3 = np.dstack([mask01]*3).astype(np.uint8)\n",
    "    bg = cv2.GaussianBlur(rgb, (15,15), 0)\n",
    "    bg = (bg.astype(np.float32) * 0.35).astype(np.uint8)\n",
    "    out = (m3 * rgb + (1 - m3) * bg).astype(np.uint8)\n",
    "    return _enc_webp(out, q=q), out\n",
    "\n",
    "def _roi_only_png(rgb, mask01):\n",
    "    bb = _tight_bbox(mask01)\n",
    "    if bb is None:\n",
    "        return _enc_webp(rgb, q=35), rgb\n",
    "    x1,y1,x2,y2 = bb\n",
    "    pad = max(4, int(0.01*max(rgb.shape[:2])))\n",
    "    x1 = max(0, x1 - pad); y1 = max(0, y1 - pad)\n",
    "    x2 = min(rgb.shape[1]-1, x2 + pad); y2 = min(rgb.shape[0]-1, y2 + pad)\n",
    "    crop = rgb[y1:y2+1, x1:x2+1]\n",
    "    return _enc_png(crop, level=3), crop\n",
    "\n",
    "@torch.no_grad()\n",
    "def adaptive_roi_bench(model, img_dir=RAW_IMG_DIR, max_images=120, threshold=BEST_T, save_preview=SAVE_FEW_SAMPLES):\n",
    "    model.eval()\n",
    "    paths = sorted(glob.glob(str(img_dir / \"*.tif\")))\n",
    "    if len(paths) == 0:\n",
    "        print(\"âš ï¸ No .tif images found\"); return None\n",
    "\n",
    "    # aggregates\n",
    "    agg = {\n",
    "        \"baseline_jpg90\": [], \"baseline_webp80\": [],\n",
    "        \"chosen_bytes\": [], \"mode\": [], \"roi_coverage\": [], \"edge_bg\": [], \"conf\": []\n",
    "    }\n",
    "    saved = 0\n",
    "\n",
    "    for p in tqdm(paths[:max_images], desc=\"ROI-Compress (adaptive)\"):\n",
    "        im_bgr = cv2.imread(p); \n",
    "        if im_bgr is None: continue\n",
    "        im_rgb = cv2.cvtColor(im_bgr, cv2.COLOR_BGR2RGB)\n",
    "        H, W = im_rgb.shape[:2]\n",
    "\n",
    "        # predict mask at 512, back to original\n",
    "        im_resz = cv2.resize(im_rgb, (512,512))\n",
    "        t = torch.from_numpy((im_resz/255.0).transpose(2,0,1)).unsqueeze(0).float().to(device)\n",
    "        prob = torch.sigmoid(model(t)).cpu().numpy()[0,0]\n",
    "        mask_small = (prob >= threshold).astype(np.uint8)\n",
    "        mask = cv2.resize(mask_small, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # metrics for policy\n",
    "        roi_cov = float(mask.mean())  # 0..1\n",
    "        conf = float(prob[mask_small==1].mean()) if mask_small.any() else 0.0\n",
    "        bg_gray = cv2.cvtColor(im_rgb, cv2.COLOR_RGB2GRAY)\n",
    "        bg_edge = _edge_density(bg_gray*(1-mask))  # edges outside ROI\n",
    "\n",
    "        # baselines\n",
    "        b_jpg = len(_enc_jpg(im_rgb, q=90))\n",
    "        b_wbp = len(_enc_webp(im_rgb, q=80))\n",
    "\n",
    "        # try candidates\n",
    "        s1, prev1 = _binary_keep_png(im_rgb, mask)\n",
    "        s2, prev2 = _soft_keep_webp(im_rgb, mask, q=60)\n",
    "        s3, prev3 = _roi_only_png(im_rgb, mask)\n",
    "        cand = {\n",
    "            \"binary_keep_png\": len(s1) if s1 is not None else 1e18,\n",
    "            \"soft_keep_webp\":  len(s2) if s2 is not None else 1e18,\n",
    "            \"roi_only_png\":    len(s3) if s3 is not None else 1e18,\n",
    "        }\n",
    "\n",
    "        # simple adaptive policy\n",
    "        # tune thresholds: ROI small < 0.12; background smooth < 0.06\n",
    "        if roi_cov < 0.12 and bg_edge < 0.06:\n",
    "            # background nearly flat, go binary PNG (huge gains)\n",
    "            mode = \"binary_keep_png\"; chosen = cand[mode]\n",
    "        elif roi_cov < 0.08:\n",
    "            # extremely small ROI â†’ send only crop\n",
    "            mode = \"roi_only_png\"; chosen = cand[mode]\n",
    "        else:\n",
    "            # keep context but cheap â†’ soft webp\n",
    "            # (optionally lower q if conf is high)\n",
    "            q = 55 if conf > 0.8 else 60\n",
    "            s2b, prev2b = _soft_keep_webp(im_rgb, mask, q=q)\n",
    "            chosen = len(s2b); mode = f\"soft_keep_webp_q{q}\"\n",
    "\n",
    "        # store\n",
    "        agg[\"baseline_jpg90\"].append(b_jpg)\n",
    "        agg[\"baseline_webp80\"].append(b_wbp)\n",
    "        agg[\"chosen_bytes\"].append(chosen)\n",
    "        agg[\"mode\"].append(mode)\n",
    "        agg[\"roi_coverage\"].append(roi_cov)\n",
    "        agg[\"edge_bg\"].append(bg_edge)\n",
    "        agg[\"conf\"].append(conf)\n",
    "\n",
    "        # tiny preview if requested\n",
    "        if save_preview>0 and saved<save_preview:\n",
    "            overlay = _overlay(im_rgb, mask)\n",
    "            # choose preview img based on mode\n",
    "            prev_img = prev2 if \"soft_keep\" in mode else (prev1 if \"binary\" in mode else prev3)\n",
    "            grid = np.concatenate([im_rgb, overlay, prev_img], axis=1)\n",
    "            cv2.imwrite(str(SAMPLES_OUT_DIR/f\"adaptive_{saved:03d}.png\"), cv2.cvtColor(grid, cv2.COLOR_RGB2BGR))\n",
    "            saved += 1\n",
    "\n",
    "    # summarize\n",
    "    agg = {k: np.array(v) if k!=\"mode\" else v for k,v in agg.items()}\n",
    "    def _summ(base):\n",
    "        return {\n",
    "            \"mean_orig_kb\": float(base.mean()/1024),\n",
    "            \"mean_out_kb\":  float(agg[\"chosen_bytes\"].mean()/1024),\n",
    "            \"saving_pct\":   float((1 - agg[\"chosen_bytes\"].mean()/base.mean())*100.0)\n",
    "        }\n",
    "\n",
    "    sum_jpg = _summ(agg[\"baseline_jpg90\"])\n",
    "    sum_web = _summ(agg[\"baseline_webp80\"])\n",
    "\n",
    "    print(\"\\n=== Adaptive Policy Results ===\")\n",
    "    print(f\"vs JPEG q=90 | mean orig: {sum_jpg['mean_orig_kb']:.1f} KB | mean out: {sum_jpg['mean_out_kb']:.1f} KB | saving: {sum_jpg['saving_pct']:.1f}%\")\n",
    "    print(f\"vs WebP q=80 | mean orig: {sum_web['mean_orig_kb']:.1f} KB | mean out: {sum_web['mean_out_kb']:.1f} KB | saving: {sum_web['saving_pct']:.1f}%\")\n",
    "\n",
    "    # mode distribution + diagnostics\n",
    "    from collections import Counter\n",
    "    cnt = Counter(agg[\"mode\"])\n",
    "    print(\"Mode distribution:\", dict(cnt))\n",
    "    print(f\"ROI coverage mean: {agg['roi_coverage'].mean():.3f} | bg edge density mean: {agg['edge_bg'].mean():.3f} | conf mean: {agg['conf'].mean():.3f}\")\n",
    "\n",
    "    return {\"summary_vs_jpeg90\": sum_jpg, \"summary_vs_webp80\": sum_web, \"modes\": cnt,\n",
    "            \"roi_cov_mean\": float(agg[\"roi_coverage\"].mean()),\n",
    "            \"edge_bg_mean\": float(agg[\"edge_bg\"].mean()),\n",
    "            \"conf_mean\": float(agg['conf'].mean())}\n",
    "\n",
    "# Run it\n",
    "adaptive_summary = adaptive_roi_bench(model, max_images=120, threshold=BEST_T, save_preview=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97751156",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T17:37:14.943896Z",
     "iopub.status.busy": "2025-11-02T17:37:14.942899Z",
     "iopub.status.idle": "2025-11-02T17:37:16.302132Z",
     "shell.execute_reply": "2025-11-02T17:37:16.301313Z",
     "shell.execute_reply.started": "2025-11-02T17:37:14.943862Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Visualization: Original | Mask Overlay | Compressed\n",
    "# ============================================\n",
    "import cv2, torch, numpy as np, matplotlib.pyplot as plt, random, glob\n",
    "from pathlib import Path\n",
    "\n",
    "RAW_IMG_DIR = Path(\"/kaggle/input/private-data-1/patches/Images\")\n",
    "SAMPLES = 3                     # number of random images to visualize\n",
    "THRESH = float(adaptive_summary[\"summary_vs_jpeg90\"][\"mean_orig_kb\"]) if \"adaptive_summary\" in globals() else 0.55\n",
    "\n",
    "def overlay_mask(rgb, mask):\n",
    "    \"\"\"Return RGB overlay of predicted mask\"\"\"\n",
    "    color = np.zeros_like(rgb)\n",
    "    color[:, :, 1] = (mask * 255)\n",
    "    return cv2.addWeighted(rgb, 1.0, color, 0.45, 0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def show_visualizations(model, img_dir=RAW_IMG_DIR, n=SAMPLES, threshold=0.55):\n",
    "    files = sorted(glob.glob(str(img_dir / \"*.tif\")))\n",
    "    chosen = random.sample(files, n)\n",
    "    model.eval()\n",
    "\n",
    "    plt.figure(figsize=(15, n * 4))\n",
    "    for i, f in enumerate(chosen):\n",
    "        im_bgr = cv2.imread(f)\n",
    "        if im_bgr is None: continue\n",
    "        im_rgb = cv2.cvtColor(im_bgr, cv2.COLOR_BGR2RGB)\n",
    "        h, w = im_rgb.shape[:2]\n",
    "\n",
    "        # Predict mask\n",
    "        inp = cv2.resize(im_rgb, (512, 512))\n",
    "        t = torch.from_numpy((inp / 255.).transpose(2, 0, 1)).unsqueeze(0).float().to(device)\n",
    "        prob = torch.sigmoid(model(t)).cpu().numpy()[0, 0]\n",
    "        mask = cv2.resize((prob >= threshold).astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # compressed version (use soft_keep_webp for visual realism)\n",
    "        m3 = np.dstack([mask]*3).astype(np.uint8)\n",
    "        bg = cv2.GaussianBlur(im_rgb, (15,15), 0)\n",
    "        bg = (bg.astype(np.float32) * 0.35).astype(np.uint8)\n",
    "        comp = (m3 * im_rgb + (1 - m3) * bg).astype(np.uint8)\n",
    "\n",
    "        # overlay\n",
    "        overlay = overlay_mask(im_rgb, mask)\n",
    "\n",
    "        # plot\n",
    "        plt.subplot(n, 3, 3*i+1); plt.imshow(im_rgb); plt.axis(\"off\"); plt.title(\"Original\")\n",
    "        plt.subplot(n, 3, 3*i+2); plt.imshow(overlay); plt.axis(\"off\"); plt.title(\"Predicted Mask Overlay\")\n",
    "        plt.subplot(n, 3, 3*i+3); plt.imshow(comp); plt.axis(\"off\"); plt.title(\"Compressed (Soft Keep WebP)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run visualization\n",
    "show_visualizations(model, threshold=0.55)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42eb9a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T17:41:15.781905Z",
     "iopub.status.busy": "2025-11-02T17:41:15.781325Z",
     "iopub.status.idle": "2025-11-02T17:42:53.031657Z",
     "shell.execute_reply": "2025-11-02T17:42:53.030795Z",
     "shell.execute_reply.started": "2025-11-02T17:41:15.781878Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# Generate pseudo-labels for unlabeled set (raw .tif)\n",
    "# ===============================================\n",
    "import torch, cv2, numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "UNLABELED_DIR = Path(\"/kaggle/input/private-data-1/patches/Images\")\n",
    "PSEUDO_DIR = Path(\"/kaggle/working/pseudo_labels\"); PSEUDO_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_pseudo_labels(model, img_dir=UNLABELED_DIR, save_dir=PSEUDO_DIR, threshold=0.55):\n",
    "    model.eval()\n",
    "    imgs = sorted(img_dir.glob(\"*.tif\"))\n",
    "    print(f\"ğŸŒ€ Generating pseudo-labels for {len(imgs)} images...\")\n",
    "    for p in tqdm(imgs):\n",
    "        name = p.stem\n",
    "        img = cv2.imread(str(p))\n",
    "        if img is None: continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        im_resz = cv2.resize(img, (512,512))\n",
    "        t = torch.from_numpy((im_resz/255.).transpose(2,0,1)).unsqueeze(0).float().to(device)\n",
    "        prob = torch.sigmoid(model(t)).cpu().numpy()[0,0]\n",
    "        np.save(save_dir/f\"{name}.npy\", prob)\n",
    "    print(f\"âœ… Saved pseudo-labels to {save_dir}\")\n",
    "\n",
    "# run\n",
    "generate_pseudo_labels(model, threshold=0.55)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715b8a46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T17:44:33.497719Z",
     "iopub.status.busy": "2025-11-02T17:44:33.497416Z",
     "iopub.status.idle": "2025-11-02T17:44:57.512430Z",
     "shell.execute_reply": "2025-11-02T17:44:57.511480Z",
     "shell.execute_reply.started": "2025-11-02T17:44:33.497698Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# FIXED: Refine pseudo-labels (shape-safe)\n",
    "# ===============================================\n",
    "import cv2, numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "UNLABELED_DIR = Path(\"/kaggle/input/private-data-1/patches/Images\")\n",
    "PSEUDO_DIR    = Path(\"/kaggle/working/pseudo_labels\")             # from generate_pseudo_labels()\n",
    "REFINE_DIR    = Path(\"/kaggle/working/pseudo_labels_refined\"); REFINE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EDGE_WEIGHT      = 0.4   # how strongly edges influence [0..1]\n",
    "SMOOTH_PROB_GAUS = 3     # Gaussian blur kernel for prob (0/None = off)\n",
    "SMOOTH_EDGE_GAUS = 3     # Gaussian blur kernel for edge map (0/None = off)\n",
    "IMP_SHARPEN_GAMMA= 1.2   # >1 boosts confident regions (0=off; 1=no change)\n",
    "\n",
    "def refine_pseudo_labels(in_dir=PSEUDO_DIR, img_dir=UNLABELED_DIR, out_dir=REFINE_DIR):\n",
    "    files = sorted(in_dir.glob(\"*.npy\"))\n",
    "    print(f\"ğŸ”§ Refining {len(files)} pseudo-labels...\")\n",
    "    bad, ok = 0, 0\n",
    "\n",
    "    for f in tqdm(files):\n",
    "        prob = np.load(f).astype(np.float32)              # (Hp, Wp) e.g. 512x512\n",
    "        Ht, Wt = prob.shape\n",
    "\n",
    "        img_path = img_dir / f\"{f.stem}.tif\"\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            bad += 1\n",
    "            continue\n",
    "\n",
    "        # Build edge map at the SAME SHAPE as prob\n",
    "        img_resz = cv2.resize(img, (Wt, Ht), interpolation=cv2.INTER_LINEAR)\n",
    "        gray = cv2.cvtColor(img_resz, cv2.COLOR_BGR2GRAY)\n",
    "        edges = cv2.Canny(gray, 100, 200).astype(np.float32) / 255.0  # (Ht, Wt)\n",
    "\n",
    "        # Optional smoothing\n",
    "        if SMOOTH_PROB_GAUS and SMOOTH_PROB_GAUS > 0:\n",
    "            k = int(SMOOTH_PROB_GAUS) | 1  # make odd\n",
    "            prob = cv2.GaussianBlur(prob, (k, k), 0)\n",
    "        if SMOOTH_EDGE_GAUS and SMOOTH_EDGE_GAUS > 0:\n",
    "            k = int(SMOOTH_EDGE_GAUS) | 1\n",
    "            edges = cv2.GaussianBlur(edges, (k, k), 0)\n",
    "\n",
    "        # Optional confidence sharpening (gamma)\n",
    "        if IMP_SHARPEN_GAMMA and IMP_SHARPEN_GAMMA > 0:\n",
    "            prob = np.clip(prob, 0, 1) ** (1.0 / float(IMP_SHARPEN_GAMMA))\n",
    "\n",
    "        # Blend with edges (align boundaries)\n",
    "        refined = (1 - EDGE_WEIGHT) * prob + EDGE_WEIGHT * edges\n",
    "        refined = np.clip(refined, 0, 1).astype(np.float32)\n",
    "\n",
    "        np.save(out_dir / f\"{f.stem}.npy\", refined)\n",
    "        ok += 1\n",
    "\n",
    "    print(f\"âœ… Refined {ok} files â†’ {out_dir}\")\n",
    "    if bad:\n",
    "        print(f\"âš ï¸ Skipped {bad} files (missing image).\")\n",
    "\n",
    "# Run it\n",
    "refine_pseudo_labels()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f6f4a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T17:45:03.183480Z",
     "iopub.status.busy": "2025-11-02T17:45:03.182510Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# Retraining using refined pseudo labels (semi-supervised)\n",
    "# ===================================================\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SemiSupervisedDataset(Dataset):\n",
    "    def __init__(self, img_dir, gt_dir, pseudo_dir):\n",
    "        self.img_dir = img_dir\n",
    "        self.gt_dir = gt_dir\n",
    "        self.pseudo_dir = pseudo_dir\n",
    "        self.files = sorted(img_dir.glob(\"*.tif\"))\n",
    "\n",
    "    def __len__(self): return len(self.files)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        path = self.files[i]; name = path.stem\n",
    "        img = cv2.imread(str(path)); img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (512,512))\n",
    "        x = torch.from_numpy(img.transpose(2,0,1)/255.).float()\n",
    "        label_path = self.gt_dir/f\"{name}.tif\"\n",
    "        if label_path.exists():\n",
    "            lbl = cv2.imread(str(label_path),0); lbl = cv2.resize(lbl,(512,512))\n",
    "            y = torch.from_numpy((lbl>128).astype(np.float32))\n",
    "        else:\n",
    "            pseudo = np.load(self.pseudo_dir/f\"{name}.npy\")\n",
    "            y = torch.from_numpy((pseudo>0.5).astype(np.float32))\n",
    "        return x, y.unsqueeze(0)\n",
    "\n",
    "dataset = SemiSupervisedDataset(\n",
    "    Path(\"/kaggle/input/private-data-1/patches/Images\"),\n",
    "    Path(\"/kaggle/input/private-data-1/patches/Labels\"),\n",
    "    REFINE_DIR\n",
    ")\n",
    "loader = DataLoader(dataset,batch_size=8,shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "EPOCHS_FINE = 10\n",
    "\n",
    "for ep in range(EPOCHS_FINE):\n",
    "    model.train(); tot=0\n",
    "    for x,y in tqdm(loader,desc=f\"Fine-tune {ep+1}/{EPOCHS_FINE}\"):\n",
    "        x,y=x.to(device),y.to(device)\n",
    "        p=torch.sigmoid(model(x))\n",
    "        loss=F.binary_cross_entropy(p,y)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        tot+=loss.item()\n",
    "    print(f\"Epoch {ep+1}: loss={tot/len(loader):.4f}\")\n",
    "torch.save(model.state_dict(),\"/kaggle/working/model_finetuned.pth\")\n",
    "print(\"âœ… Fine-tuning complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbe358a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3945689,
     "sourceId": 6865273,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 209.441658,
   "end_time": "2025-11-02T18:21:42.593442",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-02T18:18:13.151784",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
