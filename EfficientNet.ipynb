{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddb80503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa  # for potential advanced augmentations\n",
    "# import tensorflow_probability as tfp  # if using MixUp/CutMix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1a8bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Constants\\IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "NUM_CLASSES = 3\n",
    "IMAGE_SIZE= 256\n",
    "DATA_DIR = \"PlantVillage\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c03469f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5702 files belonging to 3 classes.\n",
      "Using 4562 files for training.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#----------------------------------------\n",
    "# 1. DATA LOADING\n",
    "#----------------------------------------\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=123,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00969bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5702 files belonging to 3 classes.\n",
      "Using 1140 files for validation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=123,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e705585",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#----------------------------------------\n",
    "# 2. PREPROCESSING & AUGMENTATION\n",
    "#----------------------------------------\n",
    "# 2a) Basic Resize + Rescale\n",
    "resize_and_rescale = tf.keras.Sequential([\n",
    "    tf.keras.layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    tf.keras.layers.Rescaling(1./255)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75962220",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2b) On-the-fly Augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\"),\n",
    "    tf.keras.layers.RandomRotation(factor=0.2),\n",
    "    tf.keras.layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    tf.keras.layers.RandomContrast(factor=0.2),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0cb0134",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2c) Dataset formatting function\n",
    "\n",
    "def format_train(image, label):\n",
    "    image = resize_and_rescale(image)\n",
    "    image = data_augmentation(image)\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47cce59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply mapping\n",
    "train_ds = (train_ds\n",
    "            .map(format_train, num_parallel_calls=AUTOTUNE)\n",
    "            .cache()\n",
    "            .prefetch(AUTOTUNE))\n",
    "\n",
    "val_ds = (val_ds\n",
    "          .map(lambda x, y: (resize_and_rescale(x), y), num_parallel_calls=AUTOTUNE)\n",
    "          .cache()\n",
    "          .prefetch(AUTOTUNE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ed5387c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Uncomment if using MixUp or CutMix; requires TensorFlow Probability for Beta sampling\\nimport tensorflow_probability as tfp\\n\\ndef sample_beta_distribution(size, concentration_0=0.2, concentration_1=0.2):\\n    dist = tfp.distributions.Beta(concentration_0, concentration_1)\\n    return dist.sample(sample_shape=[size])\\n\\n\\ndef mixup(images, labels, alpha=0.2):\\n    batch_size = tf.shape(images)[0]\\n    beta = sample_beta_distribution(batch_size, alpha, alpha)\\n    x_weight = tf.reshape(beta, (batch_size, 1, 1, 1))\\n    lab_weight = tf.reshape(beta, (batch_size, 1))\\n    index = tf.random.shuffle(tf.range(batch_size))\\n    mixed_images = images * x_weight + tf.gather(images, index) * (1 - x_weight)\\n    mixed_labels = labels * lab_weight + tf.gather(labels, index) * (1 - lab_weight)\\n    return mixed_images, mixed_labels\\n\\n# To integrate:\\n# train_ds = train_ds.map(lambda x, y: mixup(x, tf.one_hot(y, NUM_CLASSES)), num_parallel_calls=AUTOTUNE)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#----------------------------------------\n",
    "# 3. MIXUP / CUTMIX EXAMPLE (optional)\n",
    "#----------------------------------------\n",
    "\"\"\"\n",
    "# Uncomment if using MixUp or CutMix; requires TensorFlow Probability for Beta sampling\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "def sample_beta_distribution(size, concentration_0=0.2, concentration_1=0.2):\n",
    "    dist = tfp.distributions.Beta(concentration_0, concentration_1)\n",
    "    return dist.sample(sample_shape=[size])\n",
    "\n",
    "\n",
    "def mixup(images, labels, alpha=0.2):\n",
    "    batch_size = tf.shape(images)[0]\n",
    "    beta = sample_beta_distribution(batch_size, alpha, alpha)\n",
    "    x_weight = tf.reshape(beta, (batch_size, 1, 1, 1))\n",
    "    lab_weight = tf.reshape(beta, (batch_size, 1))\n",
    "    index = tf.random.shuffle(tf.range(batch_size))\n",
    "    mixed_images = images * x_weight + tf.gather(images, index) * (1 - x_weight)\n",
    "    mixed_labels = labels * lab_weight + tf.gather(labels, index) * (1 - lab_weight)\n",
    "    return mixed_images, mixed_labels\n",
    "\n",
    "# To integrate:\n",
    "# train_ds = train_ds.map(lambda x, y: mixup(x, tf.one_hot(y, NUM_CLASSES)), num_parallel_calls=AUTOTUNE)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81f38fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#----------------------------------------\n",
    "# 4. MODEL DEFINITION\n",
    "#----------------------------------------\n",
    "def get_model(model_name=\"EfficientNetB0\", input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), num_classes=NUM_CLASSES):\n",
    "    if model_name == \"ResNet50\":\n",
    "        base = tf.keras.applications.ResNet50(\n",
    "            include_top=False, input_shape=input_shape, weights='imagenet', pooling='avg')\n",
    "    elif model_name == \"DenseNet121\":\n",
    "        base = tf.keras.applications.DenseNet121(\n",
    "            include_top=False, input_shape=input_shape, weights='imagenet', pooling='avg')\n",
    "    elif model_name == \"MobileNetV2\":\n",
    "        base = tf.keras.applications.MobileNetV2(\n",
    "            include_top=False, input_shape=input_shape, weights='imagenet', pooling='avg')\n",
    "    else:  # Default to EfficientNetB0\n",
    "        base = tf.keras.applications.EfficientNetB0(\n",
    "            include_top=False, input_shape=input_shape, weights='imagenet', pooling='avg')\n",
    "\n",
    "    # Freeze base\n",
    "    base.trainable = False\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = base(inputs, training=False)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return tf.keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66c36016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "16705208/16705208 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and compile\n",
    "model = get_model(model_name=\"EfficientNetB0\")\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ba9ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "143/143 [==============================] - 231s 2s/step - loss: 1.1271 - accuracy: 0.3273 - val_loss: 1.0999 - val_accuracy: 0.3237\n",
      "Epoch 2/20\n",
      "143/143 [==============================] - 197s 1s/step - loss: 1.1226 - accuracy: 0.3332 - val_loss: 1.0998 - val_accuracy: 0.3237\n",
      "Epoch 3/20\n",
      "143/143 [==============================] - 197s 1s/step - loss: 1.1216 - accuracy: 0.3317 - val_loss: 1.0993 - val_accuracy: 0.3237\n",
      "Epoch 4/20\n",
      "143/143 [==============================] - 195s 1s/step - loss: 1.1261 - accuracy: 0.3345 - val_loss: 1.0997 - val_accuracy: 0.3237\n",
      "Epoch 5/20\n",
      "143/143 [==============================] - 202s 1s/step - loss: 1.1172 - accuracy: 0.3448 - val_loss: 1.0993 - val_accuracy: 0.3237\n",
      "Epoch 6/20\n",
      "143/143 [==============================] - 200s 1s/step - loss: 1.1186 - accuracy: 0.3336 - val_loss: 1.0992 - val_accuracy: 0.3237\n",
      "Epoch 7/20\n",
      "143/143 [==============================] - 194s 1s/step - loss: 1.1189 - accuracy: 0.3415 - val_loss: 1.0995 - val_accuracy: 0.3237\n",
      "Epoch 8/20\n",
      "143/143 [==============================] - 195s 1s/step - loss: 1.1192 - accuracy: 0.3330 - val_loss: 1.0990 - val_accuracy: 0.3237\n",
      "Epoch 9/20\n",
      "143/143 [==============================] - 204s 1s/step - loss: 1.1173 - accuracy: 0.3400 - val_loss: 1.0991 - val_accuracy: 0.3237\n",
      "Epoch 10/20\n",
      "143/143 [==============================] - 186s 1s/step - loss: 1.1165 - accuracy: 0.3358 - val_loss: 1.0992 - val_accuracy: 0.3237\n",
      "Epoch 11/20\n",
      "143/143 [==============================] - 188s 1s/step - loss: 1.1186 - accuracy: 0.3260 - val_loss: 1.0991 - val_accuracy: 0.3237\n",
      "Epoch 12/20\n",
      "143/143 [==============================] - 186s 1s/step - loss: 1.1178 - accuracy: 0.3338 - val_loss: 1.0994 - val_accuracy: 0.3237\n",
      "Epoch 13/20\n",
      "143/143 [==============================] - 183s 1s/step - loss: 1.1182 - accuracy: 0.3352 - val_loss: 1.0994 - val_accuracy: 0.3237\n",
      "Epoch 14/20\n",
      "143/143 [==============================] - 187s 1s/step - loss: 1.1151 - accuracy: 0.3325 - val_loss: 1.0991 - val_accuracy: 0.3237\n",
      "Epoch 15/20\n",
      "143/143 [==============================] - 185s 1s/step - loss: 1.1161 - accuracy: 0.3314 - val_loss: 1.0989 - val_accuracy: 0.3237\n",
      "Epoch 16/20\n",
      "143/143 [==============================] - 182s 1s/step - loss: 1.1128 - accuracy: 0.3428 - val_loss: 1.0990 - val_accuracy: 0.3237\n",
      "Epoch 17/20\n",
      "143/143 [==============================] - 189s 1s/step - loss: 1.1150 - accuracy: 0.3327 - val_loss: 1.0986 - val_accuracy: 0.3237\n",
      "Epoch 18/20\n",
      "143/143 [==============================] - 187s 1s/step - loss: 1.1135 - accuracy: 0.3306 - val_loss: 1.0990 - val_accuracy: 0.3237\n",
      "Epoch 19/20\n",
      "143/143 [==============================] - 188s 1s/step - loss: 1.1157 - accuracy: 0.3297 - val_loss: 1.0987 - val_accuracy: 0.3237\n",
      "Epoch 20/20\n",
      "143/143 [==============================] - 187s 1s/step - loss: 1.1156 - accuracy: 0.3266 - val_loss: 1.0988 - val_accuracy: 0.3237\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#----------------------------------------\n",
    "# 5. TRAINING\n",
    "#----------------------------------------\n",
    "EPOCHS = 20\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a73c2748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Optional: Unfreeze some layers for fine-tuning\n",
    "# base = model.layers[1]\n",
    "# base.trainable = True\n",
    "# for layer in base.layers[:-20]: layer.trainable = False\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# history_ft = model.fit(train_ds, validation_data=val_ds, epochs=10)\n",
    "\n",
    "#----------------------------------------\n",
    "# 6. SAVE MODEL\n",
    "#----------------------------------------\n",
    "model.save(\"models/plant_disease_model_EfficientNetB0.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d684b6-b99e-4f1b-b026-3cdc5771f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# 1. Constants – adjust to your values\n",
    "IMAGE_SIZE = 224  # must match IMAGE_SIZE you used during training\n",
    "MODEL_PATH = \"models/plant_disease_model_EfficientNetB0.h5\"\n",
    "CLASS_NAMES = [\n",
    "    \"Apple___Apple_scab\",\n",
    "    \"Apple___Black_rot\",\n",
    "    \"Apple___Cedar_apple_rust\",\n",
    "    \"Apple___healthy\",\n",
    "    # … all your other class names in the same order you used during training\n",
    "]\n",
    "\n",
    "from tensorflow.keras.layers import DepthwiseConv2D as _BaseDepthwiseConv2D\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# 1) Subclass the built‑in to accept & discard groups\n",
    "class DepthwiseConv2D(_BaseDepthwiseConv2D):\n",
    "    def __init__(self, *args, groups=1, **kwargs):\n",
    "        # ignore groups, pass everything else through\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "# 2) Now load with custom_objects\n",
    "MODEL_PATH = \"models/plant_disease_model_EfficientNetB0.h5\"\n",
    "model = load_model(\n",
    "    MODEL_PATH,\n",
    "    custom_objects={\"DepthwiseConv2D\": DepthwiseConv2D}\n",
    ")\n",
    "\n",
    "print(\"✅ Model loaded successfully with custom DepthwiseConv2D\")\n",
    "\n",
    "# 3. Pre‐processing helper\n",
    "def preprocess_img(img_path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Loads an image file, resizes to (IMAGE_SIZE, IMAGE_SIZE), scales pixels to [0,1],\n",
    "    and adds batch dimension.\n",
    "    \"\"\"\n",
    "    img = image.load_img(img_path, target_size=(IMAGE_SIZE, IMAGE_SIZE))\n",
    "    img_array = image.img_to_array(img)               # shape: (H, W, 3)\n",
    "    img_array /= 255.0                                # scale to [0,1]\n",
    "    return np.expand_dims(img_array, axis=0)    # 4. Single‐image inference\n",
    "test_img = \"PlantVillage/Potato___Early_blight/0a8a68ee-f587-4dea-beec-79d02e7d3fa4___RS_Early.B 8461.JPG\"  # replace with your image path  # replace with your image path\n",
    "x = preprocess_img(test_img)\n",
    "preds = model.predict(x)                    # shape: (1, num_classes)\n",
    "predicted_index = np.argmax(preds[0])\n",
    "predicted_label = CLASS_NAMES[predicted_index]\n",
    "confidence = preds[0][predicted_index]\n",
    "\n",
    "print(f\"Image: {test_img}\")\n",
    "print(f\"Predicted class: {predicted_label}  (confidence: {confidence:.3f})\")\n",
    "\n",
    "# 5. Batch inference on a folder\n",
    "test_folder = \"PlantVillage\\Potato___Early_blight\"\n",
    "for fname in os.listdir(test_folder):\n",
    "    if fname.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "        img_path = os.path.join(test_folder, fname)\n",
    "        x = preprocess_img(img_path)\n",
    "        preds = model.predict(x)\n",
    "        idx = np.argmax(preds[0])\n",
    "        label = CLASS_NAMES[idx]\n",
    "        conf = preds[0][idx]\n",
    "        print(f\"{fname:20s} → {label:30s} ({conf:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f211ff55",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No GPU found. Check your CUDA/cuDNN installation and that you're using a GPU-enabled TF build.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m gpus \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlist_physical_devices(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gpus:\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo GPU found. Check your CUDA/cuDNN installation and that you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre using a GPU-enabled TF build.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Enable “memory growth” so TF only allocates what it needs\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gpu \u001b[38;5;129;01min\u001b[39;00m gpus:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No GPU found. Check your CUDA/cuDNN installation and that you're using a GPU-enabled TF build."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# ——— 0) (Optional) Restrict visible GPUs — e.g. use only GPU 0\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# ——— 1) List and configure GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if not gpus:\n",
    "    raise RuntimeError(\"No GPU found. Check your CUDA/cuDNN installation and that you're using a GPU-enabled TF build.\")\n",
    "# Enable “memory growth” so TF only allocates what it needs\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "print(\"GPUs detected:\", gpus)\n",
    "\n",
    "# ——— 2) (Re-)load your model — with the monkey‑patch custom DepthwiseConv2D if needed\n",
    "from tensorflow.keras.layers import DepthwiseConv2D as _BaseDepthwiseConv2D\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "class DepthwiseConv2D(_BaseDepthwiseConv2D):\n",
    "    def __init__(self, *args, groups=1, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "model = load_model(\n",
    "    \"models/plant_disease_model_EfficientNetB0.h5\",\n",
    "    custom_objects={\"DepthwiseConv2D\": DepthwiseConv2D}\n",
    ")\n",
    "print(\"Model loaded. Default device:\", tf.test.gpu_device_name())\n",
    "\n",
    "# ——— 3) Wrap inference in the GPU device context\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_img(path, img_size=224):\n",
    "    img = image.load_img(path, target_size=(img_size, img_size))\n",
    "    arr = image.img_to_array(img) / 255.0\n",
    "    return np.expand_dims(arr, 0)\n",
    "\n",
    "test_img = \"PlantVillage/Potato___Early_blight/0a8a68ee-f587-4dea-beec-79d02e7d3fa4___RS_Early.B 8461.JPG\"\n",
    "x = preprocess_img(test_img)\n",
    "\n",
    "# Make sure to run on GPU:0\n",
    "with tf.device('/GPU:0'):\n",
    "    preds = model.predict(x)\n",
    "\n",
    "idx = np.argmax(preds[0])\n",
    "print(f\"Predicted: {CLASS_NAMES[idx]}  (conf: {preds[0][idx]:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "259c2179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.test.is_built_with_cuda())        # True if built with CUDA\n",
    "print(tf.config.list_physical_devices('GPU'))  # Lists available GPUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54b29108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\asus\\anaconda3\\envs\\plant-tf\\lib\\site-packages (25.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install tensorflow  # as of TF 2.15 this includes GPU support for CUDA 11.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812942ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "932ed435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plant-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
